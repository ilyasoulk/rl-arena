# ğŸ® RL Arena

A comprehensive implementation of reinforcement learning algorithms following OpenAI's Spinning Up guide. This project aims to provide clear, educational implementations of various RL algorithms while maintaining good software engineering practices.

## ğŸš€ Currently Implemented
- [x] Deep Q-Network (DQN)
  - Successfully tested on CartPole-v1 (solved in ~18k steps) and LunarLander-v3
  - Includes experience replay
  - Target network for stability
  - Epsilon-greedy exploration
  - L1 smooth loss
  - Frame stacking support

- [x] Vanilla Policy Gradient (VPG)
  - Successfully tested on CartPole-v1 (solved in ~11k steps)
  - Direct policy optimization using REINFORCE
  - Return normalization for stability
  - Stochastic action sampling for exploration
  - Frame stacking support
  - Periodic evaluation during training

## ğŸ¯ Roadmap
Planning to implement the following algorithms from OpenAI's Spinning Up (and more):
- [x] Deep-Q-Network (DQN)
- [x] Vanilla Policy Gradient (VPG)
- [ ] Trust Region Policy Optimization (TRPO)
- [ ] Proximal Policy Optimization (PPO)
- [ ] Deep Deterministic Policy Gradient (DDPG)
- [ ] Twin Delayed DDPG (TD3)
- [ ] Soft Actor-Critic (SAC)
- [ ] Group Relative Policy Optimization (GRPO)

## ğŸ› ï¸ Installation

```bash
# Clone the repository
git clone https://github.com/ilyasoulkd/rl-arena.git
cd rl-arena

# Install dependencies
uv sync
source .venv/bin/activate
```

## ğŸƒâ€â™‚ï¸ Running Experiments

### DQN on CartPole

```bash
./launch_cartpole_dqn.sh
```

Or manually with custom parameters:

```bash
python src/main.py \
    --method DQN \
    --env_name CartPole-v1 \
    --hidden_dim 128 \
    --steps 50000 \
    --capacity 10000 \
    --epsilon 1 \
    --update_frequency 1000 \
    --decay 0.001 \
    --min_eps 0.01 \
    --batch_size 32 \
    --gamma 0.99 \
    --lr 0.001 \
    --output_dir models \
    --solved_threshold 475
```

### VPG on CartPole

```bash
./launch_cartpole_vpg.sh
```

Or manually with custom parameters:

```bash
python src/main.py \
    --method VPG \
    --env_name CartPole-v1 \
    --hidden_dim 128 \
    --steps 50000 \
    --gamma 0.99 \
    --lr 0.001 \
    --num_frame_stack 1 \
    --solved_threshold 475 \
    --output_dir models
```

## ğŸ“ Project Structure

```
Directory structure:
â””â”€â”€ ilyasoulk-rl-arena/
    â”œâ”€â”€ README.md
    â”œâ”€â”€ LICENSE
    â”œâ”€â”€ launch_carracing_dqn.sh
    â”œâ”€â”€ launch_cartpole_dqn.sh
    â”œâ”€â”€ launch_cartpole_vpg.sh
    â”œâ”€â”€ launch_lunarlander_dqn.sh
    â”œâ”€â”€ launch_pong_dqn.sh
    â”œâ”€â”€ pyproject.toml
    â”œâ”€â”€ uv.lock
    â”œâ”€â”€ configs/
    â”‚   â””â”€â”€ envs.json
    â”œâ”€â”€ models/
    â”‚   â”œâ”€â”€ DQN-CartPole-v1.pth
    â”‚   â”œâ”€â”€ DQN-LunarLander-v3.pth
    â”‚   â””â”€â”€ VPG-CartPole-v1.pth
    â””â”€â”€ src/
        â”œâ”€â”€ dqn.py
        â”œâ”€â”€ main.py
        â”œâ”€â”€ models.py
        â”œâ”€â”€ utils.py
        â””â”€â”€ vpg.py
```

## ğŸ”§ Technical Details

### DQN Implementation Features
- Experience replay buffer with configurable capacity
- Target network with periodic updates
- Epsilon-greedy exploration with decay
- Gradient clipping for stability
- Support for various gym environments through space detection
- Frame stacking for temporal information
- Configurable network architectures (MLP/CNN)

### VPG Implementation Features
- Direct policy optimization using REINFORCE algorithm
- Return normalization for training stability
- Stochastic action sampling using Categorical distribution
- Frame stacking support for temporal information
- Periodic evaluation during training
- Model saving when environment is solved
- Support for both discrete and continuous action spaces

## ğŸ“ˆ Performance Comparisons

| Algorithm | Environment  | Steps to Solve |
|-----------|-------------|----------------|
| DQN       | CartPole-v1 | ~18,000       |
| VPG       | CartPole-v1 | ~11,000       |

## ğŸ“š References

- [Playing Atari with Deep Reinforcement Learning](https://arxiv.org/pdf/1312.5602)
- [Policy Gradient Methods for Reinforcement Learning with Function Approximation](https://papers.nips.cc/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf)
- [OpenAI Spinning Up](https://spinningup.openai.com/)
- [Gymnasium (formerly Gym)](https://gymnasium.farama.org/)

## ğŸ“ License

MIT License
