# ğŸ® RL Arena

A comprehensive implementation of reinforcement learning algorithms following OpenAI's Spinning Up guide. This project aims to provide clear, educational implementations of various RL algorithms while maintaining good software engineering practices.

## ğŸš€ Currently Implemented
- [x] Deep Q-Network (DQN)
  - Successfully tested on CartPole-v1 and LunarLander-v3
  - Includes experience replay
  - Target network for stability
  - Epsilon-greedy exploration
  - L1 smooth loss

## ğŸ¯ Roadmap
Planning to implement the following algorithms from OpenAI's Spinning Up (and more):
- [ ] Vanilla Policy Gradient (VPG)
- [ ] Trust Region Policy Optimization (TRPO)
- [ ] Proximal Policy Optimization (PPO)
- [ ] Deep Deterministic Policy Gradient (DDPG)
- [ ] Twin Delayed DDPG (TD3)
- [ ] Soft Actor-Critic (SAC)
- [ ] Group Relative Policy Optimization (GRPO)

## ğŸ› ï¸ Installation

```bash
# Clone the repository
git clone https://github.com/ilyasoulkd/rl-arena.git
cd rl-arena

# Install dependencies
uv sync
source .venv/bin/activate

```

## ğŸƒâ€â™‚ï¸ Running Experiments

### DQN on CartPole

```bash
./launch_cartpole_dqn.sh
```

Or manually with custom parameters:

```bash
python src/dqn.py \
    --env_name CartPole-v1 \
    --hidden_dim 128 \
    --steps 50000 \
    --capacity 10000 \
    --epsilon 1 \
    --update_frequency 1000 \
    --decay 0.001 \
    --min_eps 0.01 \
    --batch_size 32 \
    --gamma 0.99 \
    --lr 0.001 \
    --output_dir models
```

### DQN on LunarLander

```bash
./launch_lunarlander_dqn.sh
```

## ğŸ“ Project Structure
```
Directory structure:
â””â”€â”€ ilyasoulk-rl-arena/
    â”œâ”€â”€ README.md
    â”œâ”€â”€ LICENSE
    â”œâ”€â”€ launch_carracing_dqn.sh # Launch DQN CarRacing experiment
    â”œâ”€â”€ launch_cartpole_dqn.sh # CartPole-v1
    â”œâ”€â”€ launch_lunarlander_dqn.sh # LunarLander-v3
    â”œâ”€â”€ pyproject.toml # Project dependencies
    â”œâ”€â”€ uv.lock # Dependency lock
    â”œâ”€â”€ configs/
    â”‚   â””â”€â”€ envs.json # Env configs, parameters...
    â”œâ”€â”€ models/ # Models per env, currently only DQN models...
    â”‚   â”œâ”€â”€ CartPole-v1.pth
    â”‚   â””â”€â”€ LunarLander-v3.pth
    â””â”€â”€ src/
        â”œâ”€â”€ dqn.py # DQN implementation
        â”œâ”€â”€ models.py # Model architecture
        â””â”€â”€ utils.py # Utils function for env configs, experience replay, action obs space inference.

```

## ğŸ”§ Technical Details

### DQN Implementation Features
- Experience replay buffer with configurable capacity
- Target network with periodic updates
- Epsilon-greedy exploration with decay
- Gradient clipping for stability
- Support for various gym environments through space detection

## ğŸ“š References

- [Playing Atari with Deep Reinforcement Learning](https://arxiv.org/pdf/1312.5602)
- [OpenAI Spinning Up](https://spinningup.openai.com/)
- [Gymnasium (formerly Gym)](https://gymnasium.farama.org/)

## ğŸ“ License

MIT License
